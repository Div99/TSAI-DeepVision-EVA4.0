{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralArchitecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/NeuralArchitecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# call upon all the gods\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn # torch neural network\n",
        "import torch.nn.functional as F # torch functions\n",
        "import torch.optim as optim # optimizer\n",
        "from torchvision import datasets, transforms # datasets and transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"nn.Module: Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\n",
        "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()                     # format - <channels> x <rows> x <cols>\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)     # input - 1x28x28   | output - 32x28x28     | RF - 3x3\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)    # input - 32x28x28  | output - 64x28x28     | RF - 5x5\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)                 # input - 64x28x28  | output - 64x14x14     | RF - 10x10\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)   # input - 64x14x14  | output - 128x14x14    | RF - 12x12\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)  # input - 128x14x14 | output - 256x14x14    | RF - 14x14\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)                 # input - 256x14x14 | output - 256x7x7      | RF - 28x28\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)             # input - 256x7x7   | output - 512x5x5      | RF - 30x30\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)            # input - 512x5x5   | output - 1024x3x3     | RF - 32x32\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)             # input - 1024x3x3  | output - 10x1x1       | RF - 34x34\n",
        "\n",
        "# code that i added\n",
        "        # self.fc1 = nn.Linear(9216, 128)\n",
        "        # self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    \"\"\"forward\n",
        "    Defines the computation performed at every call.\n",
        "\n",
        "    Args:\n",
        "        x: the input\n",
        "\n",
        "    Returns:\n",
        "        log_softmax(x)\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))) # performs conv1 -> relu -> conv2 -> relu -> pool1\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x))))) # performs conv3 -> relu -> conv4 -> relu -> pool2\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))             # performs conv5 -> relu -> conv6 -> relu\n",
        "        x = F.sigmoid(self.conv7(x))                                 # performs conv7 -> relu\n",
        "        x = x.view(-1, 10)                                        # similar to reshape in numpy\n",
        "\n",
        "# code that i added\n",
        "        # x = torch.flatten(x, 1)\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=0)                            # perform a softmax along the 10 outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cdQgyU6IzYeI",
        "colab": {}
      },
      "source": [
        "class NetV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetV2, self).__init__()                   # format - <channels> x <rows> x <cols>\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)     # input - 1x28x28   | output - 32x28x28     | RF - 3x3\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)    # input - 32x28x28  | output - 64x28x28     | RF - 5x5\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)                 # input - 64x28x28  | output - 64x14x14     | RF - 10x10\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)   # input - 64x14x14  | output - 128x14x14    | RF - 12x12\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)                 # input - 128x14x14 | output - 128x7x7      | RF - 24x24\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3)             # input - 128x7x7   | output - 256x5x5      | RF - 26x26\n",
        "        self.conv5 = nn.Conv2d(256, 10, 5)              # input - 256x5x5   | output - 10x1x1       | RF - 28x38\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        x = self.pool2(F.relu(self.conv3(x)))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2-aN6U54gE4",
        "colab_type": "code",
        "outputId": "7ea89ab7-7a5b-43e7-9f82-d4e5feea742a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install torchsummary"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "outputId": "51561121-4d1f-4c96-bf17-0a06ff294848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# check if nvidia cuda gpu is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# transfer the model to the device chosen above\n",
        "model = Net().to(device)\n",
        "\n",
        "# print the summary of the model\n",
        "summary(model, input_size=(1, 28, 28), batch_size=-1, device='cuda')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seed the model to obtain consistent results\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# set the batch size, preferably to 2^x values\n",
        "batch_size = 128\n",
        "\n",
        "# this makes sure that the data stays in the memory\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# load the train data and perform standard normalization\n",
        "# Normalize does the following for each channel:\n",
        "# image = (image - mean) / std\n",
        "# The parameters mean, std\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "# load the test data and perform standard normalization\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "# from tqdm.auto import tqdm, trange\n",
        "\n",
        "\"\"\"trains the model\n",
        "\n",
        "Args\n",
        "    model: the model to be trained\n",
        "    device: the device on which to be trained, cpu/gpu\n",
        "    train_loader: the train data loader from torch.utils.data.DataLoader\n",
        "    optimizer: the optimizer to use for training\n",
        "    epoch: the number of epoch to run for\n",
        "\n",
        "Returns\n",
        "    None\n",
        "\"\"\"\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # set the model on train mode\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "\n",
        "        # move the data to the device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get the model output for the data\n",
        "        output = model(data)\n",
        "\n",
        "        # loss is negative log likeli-hood\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # flow the gradients backward\n",
        "        loss.backward()\n",
        "\n",
        "        # optimizer.step is performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule\n",
        "        optimizer.step()\n",
        "\n",
        "        # this is just for pretty printing\n",
        "        pbar.set_description(desc= f'loss={loss.item():.9f} batch_id={batch_idx:05d}')\n",
        "\n",
        "\"\"\"tests the model\n",
        "\n",
        "Args\n",
        "    model: the model to test\n",
        "    device: the device to use\n",
        "    test_loader: the test data loader from torch.utils.data.DataLoader\n",
        "\"\"\"\n",
        "def test(model, device, test_loader):\n",
        "\n",
        "    # set the model on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # set the test loss to zero\n",
        "    test_loss = 0\n",
        "\n",
        "    # number of correct classifications\n",
        "    correct = 0\n",
        "\n",
        "    # turn off gradients, since we are in test mode\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # move the data to device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # get the model output\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "outputId": "a9d739f2-1772-4a10-b5f0-382dbc119c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# move the model to device\n",
        "model = Net().to(device)\n",
        "# stochastic gradient descent with model parameters, learning rate and momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "# run the model for range number of times\n",
        "for epoch in range(1, 2):\n",
        "    # train the model\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # test the model\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "loss=3.763615608 batch_id=00468: 100%|██████████| 469/469 [00:18<00:00, 25.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 4.0605, Accuracy: 9566/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uru2G8tK_Y0",
        "colab_type": "text"
      },
      "source": [
        "# Memory Pinning\n",
        "\n",
        "Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See Use pinned memory buffers for more details on when and how to use pinned memory generally.\n",
        "\n",
        "For data loading, passing ``pin_memory=True`` to a DataLoader will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUe7D3MSLBI9",
        "colab_type": "text"
      },
      "source": [
        "Read on how a FC layer can be converted to a CONV layer : http://cs231n.github.io/convolutional-networks/#convert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRMcJsPHeYzX",
        "colab_type": "text"
      },
      "source": [
        "Converting FC layers to CONV layers\n",
        "It is worth noting that the only difference between FC and CONV layers is that the neurons in the CONV layer are connected only to a local region in the input, and that many of the neurons in a CONV volume share parameters. However, the neurons in both layers still compute dot products, so their functional form is identical. Therefore, it turns out that it’s possible to convert between FC and CONV layers:\n",
        "\n",
        "For any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the blocks are equal (due to parameter sharing).\n",
        "Conversely, any FC layer can be converted to a CONV layer. For example, an FC layer with K=4096 that is looking at some input volume of size 7×7×512 can be equivalently expressed as a CONV layer with F=7,P=0,S=1,K=4096. In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be 1×1×4096 since only a single depth column “fits” across the input volume, giving identical result as the initial FC layer.\n",
        "FC->CONV conversion. Of these two conversions, the ability to convert an FC layer to a CONV layer is particularly useful in practice. Consider a ConvNet architecture that takes a 224x224x3 image, and then uses a series of CONV layers and POOL layers to reduce the image to an activations volume of size 7x7x512 (in an AlexNet architecture that we’ll see later, this is done by use of 5 pooling layers that downsample the input spatially by a factor of two each time, making the final spatial size 224/2/2/2/2/2 = 7). From there, an AlexNet uses two FC layers of size 4096 and finally the last FC layers with 1000 neurons that compute the class scores. We can convert each of these three FC layers to CONV layers as described above:\n",
        "\n",
        "Replace the first FC layer that looks at [7x7x512] volume with a CONV layer that uses filter size F=7, giving output volume [1x1x4096].\n",
        "Replace the second FC layer with a CONV layer that uses filter size F=1, giving output volume [1x1x4096]\n",
        "Replace the last FC layer similarly, with F=1, giving final output [1x1x1000]\n",
        "Each of these conversions could in practice involve manipulating (e.g. reshaping) the weight matrix W in each FC layer into CONV layer filters. It turns out that this conversion allows us to “slide” the original ConvNet very efficiently across many spatial positions in a larger image, in a single forward pass.\n",
        "\n",
        "For example, if 224x224 image gives a volume of size [7x7x512] - i.e. a reduction by 32, then forwarding an image of size 384x384 through the converted architecture would give the equivalent volume in size [12x12x512], since 384/32 = 12. Following through with the next 3 CONV layers that we just converted from FC layers would now give the final volume of size [6x6x1000], since (12 - 7)/1 + 1 = 6. Note that instead of a single vector of class scores of size [1x1x1000], we’re now getting an entire 6x6 array of class scores across the 384x384 image.\n",
        "\n",
        "Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time.\n",
        "\n",
        "Naturally, forwarding the converted ConvNet a single time is much more efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation. This trick is often used in practice to get better performance, where for example, it is common to resize an image to make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the class scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m69Pf4cEejFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}